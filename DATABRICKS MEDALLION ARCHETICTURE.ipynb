{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "class GoogleCloudStorageClient:\n",
    "    def __init__(self, key_file_path: str):\n",
    "        self.credentials = self._authenticate_with_service_account(key_file_path)\n",
    "        self.storage_client = self._initialize_storage_client()\n",
    "\n",
    "    def _authenticate_with_service_account(self, key_file_path: str) -> Credentials:\n",
    "        if not os.path.exists(key_file_path):\n",
    "            raise FileNotFoundError(f\"Service account key file not found at: {key_file_path}\")\n",
    "        \n",
    "        return Credentials.from_service_account_file(key_file_path)\n",
    "\n",
    "    def _initialize_storage_client(self) -> storage.Client:\n",
    "        return storage.Client(credentials=self.credentials, project=self.credentials.project_id)\n",
    "\n",
    "# Usage\n",
    "key_file_path = \"/Workspace/Users/emmanuelihetu750@gmail.com/icezy.json\"\n",
    "gcs_client = GoogleCloudStorageClient(key_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from typing import List\n",
    "\n",
    "class GoogleCloudStorageClient:\n",
    "    def __init__(self, key_file_path: str):\n",
    "        self.credentials = self._authenticate_with_service_account(key_file_path)\n",
    "        self.storage_client = self._initialize_storage_client()\n",
    "\n",
    "    def _authenticate_with_service_account(self, key_file_path: str) -> storage.Client:\n",
    "        \"\"\"Authenticate with the service account key file.\"\"\"\n",
    "        if not os.path.exists(key_file_path):\n",
    "            raise FileNotFoundError(f\"Service account key file not found at: {key_file_path}\")\n",
    "        \n",
    "        return storage.Client.from_service_account_json(key_file_path)\n",
    "\n",
    "    def _initialize_storage_client(self) -> storage.Client:\n",
    "        \"\"\"Initialize the Google Cloud Storage client.\"\"\"\n",
    "        return storage.Client(credentials=self.credentials, project=self.credentials.project_id)\n",
    "\n",
    "    def list_buckets(self) -> List[str]:\n",
    "        \"\"\"List all buckets in the GCS project.\"\"\"\n",
    "        return [bucket.name for bucket in self.storage_client.list_buckets()]\n",
    "\n",
    "    def get_bucket(self, bucket_name: str) -> storage.Bucket:\n",
    "        \"\"\"Retrieve a specific bucket by name.\"\"\"\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            raise ValueError(f\"Bucket '{bucket_name}' does not exist.\")\n",
    "        return bucket\n",
    "\n",
    "    def list_files_in_bucket(self, bucket_name: str) -> List[str]:\n",
    "        \"\"\"List all files in the specified bucket.\"\"\"\n",
    "        bucket = self.get_bucket(bucket_name)\n",
    "        return [blob.name for blob in bucket.list_blobs()]\n",
    "\n",
    "# Usage\n",
    "key_file_path = \"/Workspace/Users/emmanuelihetu750@gmail.com/icezy.json\"\n",
    "gcs_client = GoogleCloudStorageClient(key_file_path)\n",
    "\n",
    "# List all buckets\n",
    "bucket_names = gcs_client.list_buckets()\n",
    "for name in bucket_names:\n",
    "    print(f\"Bucket: {name}\")\n",
    "\n",
    "# List all files in a specific bucket\n",
    "bucket_name = 'keyrus_data'\n",
    "file_names = gcs_client.list_files_in_bucket(bucket_name)\n",
    "for file_name in file_names:\n",
    "    print(f\"File: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class DirectoryManager:\n",
    "    def __init__(self, directory_path: str):\n",
    "        self.directory_path = Path(directory_path)\n",
    "        self.create_directory()\n",
    "\n",
    "    def create_directory(self):\n",
    "        \"\"\"Create the directory if it does not exist.\"\"\"\n",
    "        try:\n",
    "            self.directory_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Directory created at: {self.directory_path}\")\n",
    "        except OSError as e:\n",
    "            raise RuntimeError(f\"Failed to create directory at {self.directory_path}: {e}\")\n",
    "\n",
    "class DatabricksFileSystem:\n",
    "    @staticmethod\n",
    "    def copy_to_dbfs(local_path: str, dbfs_path: str):\n",
    "        \"\"\"Copy a file from local filesystem to DBFS.\"\"\"\n",
    "        try:\n",
    "            dbutils.fs.cp(f\"file://{local_path}\", dbfs_path)\n",
    "            print(f\"File copied to DBFS at: {dbfs_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to copy file to DBFS: {e}\")\n",
    "\n",
    "# Usage\n",
    "directory_path = '/dbfs/tmp/'\n",
    "local_file_path = '/local/path/to/keyrus_sales.csv'\n",
    "dbfs_file_path = 'dbfs:/tmp/keyrus_sales.csv'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "DirectoryManager(directory_path)\n",
    "\n",
    "# Copy the file to DBFS\n",
    "DatabricksFileSystem.copy_to_dbfs(local_file_path, dbfs_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f651a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class SparkCSVLoader:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def load_csv_to_df(self, file_path: str, header: bool = True, infer_schema: bool = True) -> DataFrame:\n",
    "        \"\"\"Load a CSV file from DBFS into a Spark DataFrame.\"\"\"\n",
    "        try:\n",
    "            df = self.spark.read.csv(file_path, header=header, inferSchema=infer_schema)\n",
    "            print(f\"CSV file loaded into DataFrame from: {file_path}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load CSV into DataFrame: {e}\")\n",
    "\n",
    "    def display_df(self, df: DataFrame, num_rows: int = 20):\n",
    "        \"\"\"Display the content of the DataFrame.\"\"\"\n",
    "        try:\n",
    "            df.show(num_rows)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to display DataFrame content: {e}\")\n",
    "\n",
    "# Usage\n",
    "spark = SparkSession.builder.appName(\"CSVLoaderApp\").getOrCreate()\n",
    "csv_loader = SparkCSVLoader(spark)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = \"dbfs:/tmp/keyrus_sales.csv\"\n",
    "data_df = csv_loader.load_csv_to_df(file_path)\n",
    "\n",
    "# Show the content of the DataFrame\n",
    "csv_loader.display_df(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50755ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class DataFrameCleaner:\n",
    "    @staticmethod\n",
    "    def clean_column_name(col_name: str) -> str:\n",
    "        \"\"\"Clean a column name by replacing invalid characters with underscores.\"\"\"\n",
    "        return re.sub(r'[ ,;{}()\\n\\t=]', '_', col_name)\n",
    "\n",
    "    def clean_column_names(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Apply the cleaning function to all column names in the DataFrame.\"\"\"\n",
    "        cleaned_columns = [df[col].alias(self.clean_column_name(col)) for col in df.columns]\n",
    "        cleaned_df = df.select(cleaned_columns)\n",
    "        print(\"Column names cleaned.\")\n",
    "        return cleaned_df\n",
    "\n",
    "# Usage\n",
    "df_cleaner = DataFrameCleaner()\n",
    "\n",
    "# Clean the column names in the DataFrame\n",
    "cleaned_data_df = df_cleaner.clean_column_names(data_df)\n",
    "\n",
    "# Verify the cleaned column names\n",
    "print(\"Cleaned Columns:\", cleaned_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "class DeltaTableManager:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def save_as_delta(self, df: DataFrame, save_path: str, mode: str = \"overwrite\"):\n",
    "        \"\"\"Save the DataFrame as a Delta table at the specified path.\"\"\"\n",
    "        try:\n",
    "            df.write.format(\"delta\").mode(mode).save(save_path)\n",
    "            print(f\"Data successfully saved in Delta format at: {save_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save DataFrame as Delta: {e}\")\n",
    "\n",
    "    def load_delta_table(self, load_path: str) -> DataFrame:\n",
    "        \"\"\"Load a Delta table from the specified path into a DataFrame.\"\"\"\n",
    "        try:\n",
    "            df = self.spark.read.format(\"delta\").load(load_path)\n",
    "            print(f\"Delta table loaded from: {load_path}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load Delta table: {e}\")\n",
    "\n",
    "    def create_temp_view(self, df: DataFrame, view_name: str):\n",
    "        \"\"\"Create a temporary view for the DataFrame.\"\"\"\n",
    "        try:\n",
    "            df.createOrReplaceTempView(view_name)\n",
    "            print(f\"Temporary view '{view_name}' created.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create temporary view: {e}\")\n",
    "\n",
    "    def query_view(self, query: str):\n",
    "        \"\"\"Run a SQL query on a temporary view and display the results.\"\"\"\n",
    "        try:\n",
    "            result_df = self.spark.sql(query)\n",
    "            result_df.show()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to execute query: {e}\")\n",
    "\n",
    "# Usage\n",
    "spark = SparkSession.builder.appName(\"DeltaTableApp\").getOrCreate()\n",
    "delta_manager = DeltaTableManager(spark)\n",
    "\n",
    "# Path to save the Bronze layer data\n",
    "bronze_path = \"dbfs:/mnt/bronze/keyrus_sales\"\n",
    "\n",
    "# Save the cleaned data to the Bronze table in Delta format\n",
    "delta_manager.save_as_delta(cleaned_data_df, bronze_path)\n",
    "\n",
    "# Load the Delta table\n",
    "bronze_df = delta_manager.load_delta_table(bronze_path)\n",
    "\n",
    "# Create a temporary view for the Bronze layer\n",
    "delta_manager.create_temp_view(bronze_df, \"bronze_sales\")\n",
    "\n",
    "# Query the temporary view\n",
    "delta_manager.query_view(\"SELECT * FROM bronze_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class SilverLayerProcessor:\n",
    "    def __init__(self, delta_manager: DeltaTableManager):\n",
    "        self.delta_manager = delta_manager\n",
    "\n",
    "    def process_silver_layer(self, bronze_df: DataFrame, silver_path: str) -> DataFrame:\n",
    "        \"\"\"Process the Silver layer by filtering out cancelled orders and calculating the total amount.\"\"\"\n",
    "        try:\n",
    "            silver_df = bronze_df.filter(col(\"Order_Status\") != \"Cancelled\") \\\n",
    "                                 .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Unit_Price\"))\n",
    "            print(\"Silver layer processed.\")\n",
    "            \n",
    "            # Save the processed data to the Silver Delta table\n",
    "            self.delta_manager.save_as_delta(silver_df, silver_path)\n",
    "            return silver_df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to process Silver layer: {e}\")\n",
    "\n",
    "# Usage\n",
    "silver_path = \"dbfs:/mnt/silver/keyrus_sales\"\n",
    "silver_processor = SilverLayerProcessor(delta_manager)\n",
    "\n",
    "# Process the Silver layer\n",
    "silver_data_df = silver_processor.process_silver_layer(bronze_df, silver_path)\n",
    "\n",
    "# Create a temporary view for the Silver layer\n",
    "delta_manager.create_temp_view(silver_data_df, \"silver_sales\")\n",
    "\n",
    "# Query the Silver layer\n",
    "delta_manager.query_view(\"SELECT * FROM silver_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456448bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "class GoldLayerProcessor:\n",
    "    def __init__(self, delta_manager: DeltaTableManager):\n",
    "        self.delta_manager = delta_manager\n",
    "\n",
    "    def process_gold_layer(self, silver_df: DataFrame, gold_path: str) -> DataFrame:\n",
    "        \"\"\"Process the Gold layer by aggregating total sales per product.\"\"\"\n",
    "        try:\n",
    "            # Aggregate the total sales and quantity per product\n",
    "            gold_df = silver_df.groupBy(\"Product_ID\") \\\n",
    "                               .agg(\n",
    "                                   _sum(\"TotalAmount\").alias(\"Total_Sales\"),\n",
    "                                   _sum(\"Quantity\").alias(\"Total_Quantity_Sold\")\n",
    "                               )\n",
    "            print(\"Gold layer processed.\")\n",
    "            \n",
    "            # Save the aggregated data to the Gold Delta table\n",
    "            self.delta_manager.save_as_delta(gold_df, gold_path)\n",
    "            return gold_df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to process Gold layer: {e}\")\n",
    "\n",
    "# Usage\n",
    "gold_path = \"dbfs:/mnt/gold/keyrus_sales\"\n",
    "gold_processor = GoldLayerProcessor(delta_manager)\n",
    "\n",
    "# Process the Gold layer\n",
    "gold_data_df = gold_processor.process_gold_layer(silver_data_df, gold_path)\n",
    "\n",
    "# Create a temporary view for the Gold layer\n",
    "delta_manager.create_temp_view(gold_data_df, \"gold_sales\")\n",
    "\n",
    "# Query the Gold layer\n",
    "delta_manager.query_view(\"SELECT * FROM gold_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- SQL Query to retrieve the top 10 products by total sales from the Gold layer\n",
    "-- This query orders the products by their total sales in descending order\n",
    "-- and limits the result to the top 10 entries.\n",
    "\n",
    "SELECT Product_ID, Total_Sales, Total_Quantity_Sold \n",
    "FROM gold_sales \n",
    "ORDER BY Total_Sales DESC \n",
    "LIMIT 10;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
